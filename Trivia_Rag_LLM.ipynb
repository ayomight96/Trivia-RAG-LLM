{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayomight96/Trivia-RAG-LLM/blob/main/Trivia_Rag_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aee376271d614f",
      "metadata": {
        "collapsed": false,
        "id": "2aee376271d614f"
      },
      "source": [
        "# Trivia-Rag-LLM\n",
        "\n",
        "This code presents a RAG-LLM model, that should potentially take a set of questions in a csv file and return an output of the answer in another CSV file. The following packages were installed:\n",
        "1. transformers by hugging face\n",
        "2. torch\n",
        "3. scikit-learn\n",
        "4. accelerate\n",
        "5. faiss-gpu\n",
        "6. sentence-transformers\n",
        "\n",
        "Also the following libraries were imported:\n",
        "1. pandas\n",
        "2. torch\n",
        "3. SentenceTransformer\n",
        "4. AutoTokenizer\n",
        "5. AutoModel\n",
        "6. tqdm\n",
        "7. numpy\n",
        "8. faiss\n",
        "9. google drive\n",
        "10. AutoModelForCausalLM\n",
        "11. and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dce671fad498e247",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-29T19:17:03.677905Z",
          "start_time": "2023-10-29T19:16:41.839356Z"
        },
        "scrolled": true,
        "id": "dce671fad498e247",
        "outputId": "74b1d9e6-8b99-4d2b-bfce-6a0eb8b2a84a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.48.0.dev0\n",
            "Uninstalling transformers-4.48.0.dev0:\n",
            "  Successfully uninstalled transformers-4.48.0.dev0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-4kyq_bhy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-4kyq_bhy\n",
            "  Resolved https://github.com/huggingface/transformers to commit 6eb00dd2f0283f46d21ce9466d8d4e21dfd02550\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2024.8.30)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10236311 sha256=f952626e064dc581972cddb5d81eb15c1c0a18bff78b48899d31a3b0bb705c1c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nalignox/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.48.0.dev0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: accelerate==0.31.0 in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (0.26.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.31.0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.31.0) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.31.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.31.0) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.31.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.31.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.31.0) (2024.8.30)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.48.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "#!pip install openai==0.28\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install accelerate==0.31.0 #install for fix error \"cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub'\"\n",
        "!pip install faiss-gpu\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcc2a97-50f3-4557-80bf-575a10e1f289",
      "metadata": {
        "id": "5dcc2a97-50f3-4557-80bf-575a10e1f289"
      },
      "source": [
        "## The data\n",
        "\n",
        "A total of 53800 trivia questions with their answers were pulled from multiple sources, web scraping, and some datasets from kaggle. The information was put together into one excel document which is then uploaded to google drive for ease of access within collab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec0ad1e-2460-4e9a-ba5d-2ef2dfa1f4df",
      "metadata": {
        "id": "3ec0ad1e-2460-4e9a-ba5d-2ef2dfa1f4df"
      },
      "source": [
        "## Preprocessing of the data\n",
        "\n",
        "Because of the nature of the trivia questions and answers, they have to be concatenated to form a more contextual sentence for easy processing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Excel file\n",
        "df = pd.read_excel(\"/content/drive/My Drive/LLM/Trivia_new.xlsx\")\n",
        "\n",
        "# Convert rows into sentences\n",
        "def row_to_sentence(row):\n",
        "    return f\"The question is: {row['Question']}. The answer is: {row['Answer']}.\"\n",
        "\n",
        "# THIS WAS ALREADY DONE AND MOUNTED UNTO MY GOOGLE DRIVE\n",
        "\n",
        "sentences = df.apply(row_to_sentence, axis=1).tolist()\n",
        "\n",
        "\n",
        "documents = [\n",
        "  \"On 14 April, ESA launched the Jupiter Icy Moons Explorer (JUICE) spacecraft to explore Jupiter and its large ice-covered moons following an eight-year transit.\",\n",
        "  \"ISRO launched its third lunar mission Chandrayaan-3 on 14 July 2023 at 9:05 UTC; it consists of lander, rover and a propulsion module, and successfully landed in the south pole region of the Moon on 23 August 2023.\",\n",
        "  \"Russian lunar lander Luna 25 was launched on 10 August 2023, 23:10 UTC, atop a Soyuz-2.1b rocket from the Vostochny Cosmodrome, it was the first Russian attempt to land a spacecraft on the Moon since the Soviet lander Luna 24 in 1974, it crashed on the Moon on 19 August after technical glitches.\",\n",
        "  \"JAXA launched SLIM (Smart Lander for Investigating Moon) lunar lander (carrying a mini rover) and a space telescope (XRISM) on 6 September.\",\n",
        "  \"The OSIRIS-REx mission returned to Earth on 24 September with samples collected from asteroid Bennu.\",\n",
        "  \"NASA launched the Psyche spacecraft on 13 October 2023, an orbiter mission that will explore the origin of planetary cores by studying the metallic asteroid 16 Psyche, on a Falcon Heavy launch vehicle.\"\n",
        "]\n",
        "documents.extend(sentences)\n",
        "\n",
        "#Documents have been uploaded already so all I need to do is download.\n",
        "# Load document_index from Google Drive\n",
        "loaded_document_index = np.load(\"/content/drive/My Drive/LLM/document_index.npy\").astype(np.float32)\n",
        "print(f\"Loaded Document shape: {loaded_document_index.shape}\")\n",
        "loaded_document_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2GRkhDJRjnm",
        "outputId": "80a00c50-b694-4ea4-ed90-70a449445ca5"
      },
      "id": "t2GRkhDJRjnm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded embeddings shape: (58300, 768)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58300, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "\n",
        "To ensure smooth and easy retrieval of the large trivia data the following methods were written."
      ],
      "metadata": {
        "id": "R430S-6FSTXl"
      },
      "id": "R430S-6FSTXl"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12ef6dc6-1b14-4023-b38a-abec745fd1c0",
      "metadata": {
        "id": "12ef6dc6-1b14-4023-b38a-abec745fd1c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee360a8-69fe-49cd-ced8-698c71c3d952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def embed_documents(docs, model_name, batch_size=16, use_gpu=True):\n",
        "  \"\"\"Embed the provided documents to create a document index\"\"\"\n",
        "  # load the tokenizer and model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "  model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Initialize list to store embeddings\n",
        "  device = torch.device(\"cuda\")\n",
        "  all_embeddings = []\n",
        "\n",
        "  # Use SentenceTransformer model for mean-pooled embeddings\n",
        "  model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "  # Process documents in batches\n",
        "  for i in tqdm(range(0, len(docs), batch_size), desc=\"Embedding Documents\"):\n",
        "        batch_docs = docs[i:i + batch_size]\n",
        "        batch_embeddings = model.encode(batch_docs, batch_size=batch_size, show_progress_bar=False)\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "\n",
        "  # Concatenate all embeddings\n",
        "  return np.vstack(all_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6666e8f8-061c-4bbb-964a-f7d20d86fb30",
      "metadata": {
        "id": "6666e8f8-061c-4bbb-964a-f7d20d86fb30"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "def create_faiss_index(doc_index):\n",
        "    \"\"\"\n",
        "    Create a FAISS index for efficient similarity search.\n",
        "\n",
        "    Parameters:\n",
        "    - doc_index (np.ndarray): Precomputed document embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - faiss.IndexFlatL2: The FAISS index.\n",
        "    \"\"\"\n",
        "    dimension = doc_index.shape[1]  # Embedding dimension\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(doc_index)  # Add embeddings to the FAISS index\n",
        "    return index\n",
        "\n",
        "def retrieve_documents(query_string, faiss_index, docs, model_name=\"BAAI/bge-base-en\", k=5):\n",
        "    \"\"\"\n",
        "    Retrieve the top-k most similar documents using FAISS.\n",
        "\n",
        "    Parameters:\n",
        "    - query_string (str): The query text.\n",
        "    - faiss_index (faiss.IndexFlatL2): The FAISS index.\n",
        "    - docs (list): List of original documents.\n",
        "    - model_name (str): Hugging Face model name for embedding.\n",
        "    - k (int): Number of documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    - List of the top-k most similar documents.\n",
        "    \"\"\"\n",
        "    # Embed the query string\n",
        "    query_vector = embed_documents([query_string], model_name=model_name).reshape(1, -1)\n",
        "\n",
        "    # Query the FAISS index\n",
        "    distances, indices = faiss_index.search(query_vector, k)\n",
        "\n",
        "    # Retrieve top-k documents\n",
        "    return [docs[i] for i in indices[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "54d253e9-db6d-4807-b770-46eb720f6dcb",
      "metadata": {
        "id": "54d253e9-db6d-4807-b770-46eb720f6dcb",
        "outputId": "3a8c6c5d-ecdd-41d8-dfdd-cbc175630500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Load the FAISS index\n",
        "loaded_index = faiss.read_index(\"/content/drive/My Drive/LLM/faiss_index.index\")\n",
        "print(\"FAISS index loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c1ab9bad-17e1-4989-b0fb-0292f8677d55",
      "metadata": {
        "id": "c1ab9bad-17e1-4989-b0fb-0292f8677d55"
      },
      "outputs": [],
      "source": [
        "def create_augmented_prompt(query_string, docs):\n",
        "  # concatenate the retrieved docs as context for the LLM\n",
        "  # you could do other pre-processing here too\n",
        "  context = \"\\n\".join(docs)\n",
        "  # define your prompt template\n",
        "  prompt_template = \"\"\"Here is some relevant information:\n",
        "  {context}\n",
        "\n",
        "  Q: {query}\n",
        "  Provide only the correct option letter (e.g., A, B, C, or D). Do not include any explanation. And if you are not sure make an educated guess.\n",
        "  A:\n",
        "  \"\"\"\n",
        "  # render the prompt template\n",
        "  return prompt_template.format(context=context, query=query_string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "from transformers import AutoModelForCausalLM, pipeline\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 10,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.2,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "def generate_response(query_string, chosen_model,generation_arguments):\n",
        "  messages = [{\"content\": query_string, \"role\": \"user\"}]\n",
        "  output = chosen_model(messages,**generation_arguments)\n",
        "  return output[0]['generated_text'].strip()\n",
        "\n",
        "def generate_rag_response(\n",
        "    query_string,\n",
        "    docs,\n",
        "    faiss_index,\n",
        "    chosen_model=pipe,\n",
        "    generation_arguments=generation_args,\n",
        "    k=3\n",
        "):\n",
        "\n",
        "  # R: retrieve documents\n",
        "  retrieved_docs = retrieve_documents(\n",
        "      query_string, faiss_index, documents\n",
        "  )\n",
        "  # A: create augmented prompt\n",
        "  augmented_prompt = create_augmented_prompt(query_string, retrieved_docs)\n",
        "\n",
        "  # G: generate response!\n",
        "  #generated_response = generate_response(augmented_prompt, model_name)\n",
        "  generated_response = generate_response(augmented_prompt, chosen_model=chosen_model,generation_arguments=generation_arguments)\n",
        "  return generated_response\n",
        "\n",
        "def process_questions_in_batches(input_csv, output_csv, docs=documents, faiss_index=loaded_index, model=pipe, generation_args=generation_args, batch_size=3, max_retries=20):\n",
        "    \"\"\"\n",
        "    Processes questions in batches, generates answers using RAG, and appends them to a CSV file in Google Drive.\n",
        "    Restarts a batch automatically if CUDA out-of-memory occurs.\n",
        "\n",
        "    Args:\n",
        "        input_csv (str): Path to the input CSV containing 'Number' and 'Question'.\n",
        "        output_csv (str): Path to save the answered questions in Google Drive.\n",
        "        docs (list): Source document list for RAG.\n",
        "        faiss_index: FAISS index for retrieval.\n",
        "        model: LLM pipeline for response generation.\n",
        "        generation_args (dict): Arguments for the LLM.\n",
        "        batch_size (int): Number of rows to process in each batch.\n",
        "        max_retries (int): Maximum retries for a batch when CUDA out-of-memory occurs.\n",
        "    \"\"\"\n",
        "    # Ensure we start with a fresh output file\n",
        "    if os.path.exists(output_csv):\n",
        "        os.remove(output_csv)  # Remove existing file\n",
        "\n",
        "    # Track current row to retry failed batches\n",
        "    current_start_row = 0\n",
        "\n",
        "    # Process CSV in chunks\n",
        "    for chunk in pd.read_csv(input_csv, chunksize=batch_size, skiprows=range(1, current_start_row + 1)):\n",
        "        retries = 0  # Track retries for the current batch\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                print(f\"Processing batch starting from row {current_start_row}\")\n",
        "                batch_results = []  # Temporary storage for batch results\n",
        "\n",
        "                # Iterate over the rows in the chunk\n",
        "                for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=\"Processing Batch\"):\n",
        "                    question_number = row['Number']\n",
        "                    question_text = row['Question']\n",
        "\n",
        "                    # Generate RAG response\n",
        "                    response = generate_rag_response(\n",
        "                        query_string=question_text,\n",
        "                        docs=docs,\n",
        "                        faiss_index=faiss_index,\n",
        "                        chosen_model=model,\n",
        "                        generation_arguments=generation_args\n",
        "                    )\n",
        "\n",
        "                    # Extract and clean the response (only the first word/letter)\n",
        "                    clean_response = response.strip().split()[0]\n",
        "                    batch_results.append({\"Number\": question_number, \"Answer\": clean_response.strip().split()[0]})\n",
        "\n",
        "                # Save batch results to CSV (append mode)\n",
        "                batch_df = pd.DataFrame(batch_results)\n",
        "                batch_df.to_csv(output_csv, index=False, mode='a', header=not os.path.exists(output_csv))\n",
        "\n",
        "                # Clear GPU memory after each batch\n",
        "                torch.cuda.empty_cache()\n",
        "                print(f\"Batch processed and appended to {output_csv}.\")\n",
        "\n",
        "                # Move to the next chunk\n",
        "                current_start_row += batch_size\n",
        "                break  # Exit retry loop if successful\n",
        "\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                retries += 1\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory\n",
        "                print(f\"CUDA out of memory. Retrying batch ({retries}/{max_retries})...\")\n",
        "                if retries >= max_retries:\n",
        "                    print(\"Maximum retries reached. Skipping this batch.\")\n",
        "                    current_start_row += batch_size  # Skip to the next batch\n",
        "                    break  # Exit the retry loop and move to the next batch\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error: {e}\")\n",
        "                current_start_row += batch_size\n",
        "                break  # Exit the retry loop for unexpected errors\n",
        "\n",
        "    print(f\"All questions processed. Results saved to {output_csv}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "244debce7d7b431bbf14cca17655a53e",
            "cffeece650704e1f95ad1654a584321c",
            "1610c78c8080410cba6a14c0cc9a23fc",
            "75ac36c556174e27abe20a1d7e0996b2",
            "6238c0d8b8c0418a832f9d51742009b2",
            "05fe3fd85baf4314ba94f5879d31f14f",
            "015852e3a2e34fc78a5a64b9cfb75184",
            "e69d314f8bb941afa422f9430ab476f9",
            "b8d5c6fd4b66470eb2fb7d56e57ca89d",
            "4a1413c9a75349699083ba0c6769d966",
            "62ee126b4a3343f1be8fa8e1904840eb"
          ]
        },
        "id": "PVlWZq7BnrX_",
        "outputId": "b74a1a81-d948-403b-cfc0-b8fe7e843e49"
      },
      "id": "PVlWZq7BnrX_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "244debce7d7b431bbf14cca17655a53e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process CSV\n",
        "input_csv = \"/content/drive/My Drive/LLM/Hackathon_Question_set.csv\"\n",
        "output_csv = \"/content/drive/My Drive/LLM/Team01.csv\"\n",
        "process_questions_in_batches(\n",
        "    input_csv=input_csv,\n",
        "    output_csv=output_csv\n",
        ")\n",
        "\n",
        "answers = pd.read_csv(output_csv)\n",
        "answers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dnsEdQwMK4R4",
        "outputId": "770c9933-cb8f-47f5-8b56-c96182dd550c"
      },
      "id": "dnsEdQwMK4R4",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch starting from row 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:08<00:16,  8.38s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.94it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:12<00:05,  5.62s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 66.67it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:15<00:00,  5.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 33.13it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.88s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.60it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.63s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 67.08it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 47.42it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.71s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 70.61it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:10<00:05,  5.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 67.56it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.02s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.84it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.03s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 74.99it/s]\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 67.34it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.74s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 59.75it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.79s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 40.95it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 63.44it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:06,  3.45s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 55.59it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 42.24it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.49s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 67.23it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:03,  3.98s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.52it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 33.58it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.62s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.32it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.12s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 44.52it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 61.87it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.77s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 61.88it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 59.90it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.13s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 67.03it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.31s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 49.98it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.88it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.99s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 65.93it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  4.00s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 68.67it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 72.31it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.68s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 45.27it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:10<00:05,  5.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.85it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.99s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.99it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.84s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 43.78it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 54.35it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.53s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.41it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.61s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.70it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:10<00:00,  3.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 47.07it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.77s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.67it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:10<00:05,  5.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 52.93it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.34s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.53it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.20s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.65it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 38.09it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.54s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 57.03it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:04,  4.04s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 60.24it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 56.49it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.63s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 45.76it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 54.79it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.15s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 59.50it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.90s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 52.42it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 65.72it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.58s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.27it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.51s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 65.42it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.03it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.59s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 59.66it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 33.15it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:09,  4.58s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 61.41it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.41s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 45.48it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 52.14it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.33s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 53.69it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.86s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 70.10it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 60.58it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:06,  3.43s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 42.26it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.06it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.07s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 63.94it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.79s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 44.31it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.14it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.64s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 68.30it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.52s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.75it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:10<00:00,  3.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 23.24it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:05<00:10,  5.26s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 38.43it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:13<00:06,  6.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 20.56it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:06<00:12,  6.49s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 41.42it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:11<00:05,  5.45s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 50.19it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:15<00:00,  5.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 46.59it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:09,  4.76s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 63.65it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.07s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 63.58it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 36.26it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.82s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 51.66it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 45.13it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:09,  4.68s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 40.98it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.13s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 66.26it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 54.71it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.60s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 63.21it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.53s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 65.51it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 55.95it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.84s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 51.85it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:11<00:05,  5.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 51.04it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.17s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 60.89it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.88s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.61it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.79it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.06s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 62.34it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.70s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 69.92it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:10<00:00,  3.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 55.38it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.62s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 64.61it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 68.30it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.93s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 48.59it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.89s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 20.13it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 23.85it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.40s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 21.52it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:09<00:04,  4.80s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 58.53it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:13<00:00,  4.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 61.41it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.57s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 43.73it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:11<00:05,  5.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 74.50it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:05<00:10,  5.24s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 51.74it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.20s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 44.60it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 59.45it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.75s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 57.47it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.62s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 67.65it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:10<00:00,  3.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 35.12it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.05s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 58.09it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:10<00:05,  5.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 50.48it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:09,  4.58s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:10<00:05,  5.10s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 52.24it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:13<00:00,  4.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 54.23it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.31s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 43.32it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.35s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 49.55it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:13<00:00,  4.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 74.49it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.53s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 45.40it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:10<00:05,  5.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 47.41it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:08,  4.27s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 53.40it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.82s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 55.33it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 54.94it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:03<00:07,  3.83s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 57.89it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:07<00:03,  3.76s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 44.73it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:11<00:00,  3.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 22.31it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:05<00:10,  5.49s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 47.27it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:12<00:06,  6.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (1/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (2/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (3/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (4/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (5/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (6/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (7/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (8/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (9/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA out of memory. Retrying batch (10/20)...\n",
            "Processing batch starting from row 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 46.66it/s]\n",
            "Processing Batch:  33%|███▎      | 1/3 [00:04<00:09,  4.88s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 65.73it/s]\n",
            "Processing Batch:  67%|██████▋   | 2/3 [00:08<00:04,  4.20s/it]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 58.89it/s]\n",
            "Processing Batch: 100%|██████████| 3/3 [00:12<00:00,  4.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "Processing batch starting from row 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Embedding Documents: 100%|██████████| 1/1 [00:00<00:00, 51.56it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Processing Batch: 100%|██████████| 1/1 [00:04<00:00,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch processed and appended to /content/drive/My Drive/LLM/Team01.csv.\n",
            "All questions processed. Results saved to /content/drive/My Drive/LLM/Team01.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Number Answer\n",
              "0        1      C\n",
              "1        2      B\n",
              "2        3      B\n",
              "3        4      C\n",
              "4        5     A)\n",
              "..     ...    ...\n",
              "95      96      B\n",
              "96      97      A\n",
              "97      98      A\n",
              "98      99      B\n",
              "99     100      A\n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e01529b9-e634-4c71-b5f9-01c38dacce20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>A)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e01529b9-e634-4c71-b5f9-01c38dacce20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e01529b9-e634-4c71-b5f9-01c38dacce20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e01529b9-e634-4c71-b5f9-01c38dacce20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-649336d0-3d13-4a2e-87b6-8ad9c2f71c2f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-649336d0-3d13-4a2e-87b6-8ad9c2f71c2f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-649336d0-3d13-4a2e-87b6-8ad9c2f71c2f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_25f3e7be-4e69-44c0-a77c-e07ef2ddf545\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('answers')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_25f3e7be-4e69-44c0-a77c-e07ef2ddf545 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('answers');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "answers",
              "summary": "{\n  \"name\": \"answers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 1,\n        \"max\": 100,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          84,\n          54,\n          71\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"B\",\n          \"A\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-explorations",
      "language": "python",
      "name": "llm-explorations"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "244debce7d7b431bbf14cca17655a53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cffeece650704e1f95ad1654a584321c",
              "IPY_MODEL_1610c78c8080410cba6a14c0cc9a23fc",
              "IPY_MODEL_75ac36c556174e27abe20a1d7e0996b2"
            ],
            "layout": "IPY_MODEL_6238c0d8b8c0418a832f9d51742009b2"
          }
        },
        "cffeece650704e1f95ad1654a584321c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05fe3fd85baf4314ba94f5879d31f14f",
            "placeholder": "​",
            "style": "IPY_MODEL_015852e3a2e34fc78a5a64b9cfb75184",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1610c78c8080410cba6a14c0cc9a23fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e69d314f8bb941afa422f9430ab476f9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8d5c6fd4b66470eb2fb7d56e57ca89d",
            "value": 2
          }
        },
        "75ac36c556174e27abe20a1d7e0996b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a1413c9a75349699083ba0c6769d966",
            "placeholder": "​",
            "style": "IPY_MODEL_62ee126b4a3343f1be8fa8e1904840eb",
            "value": " 2/2 [00:31&lt;00:00, 15.05s/it]"
          }
        },
        "6238c0d8b8c0418a832f9d51742009b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05fe3fd85baf4314ba94f5879d31f14f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "015852e3a2e34fc78a5a64b9cfb75184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e69d314f8bb941afa422f9430ab476f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d5c6fd4b66470eb2fb7d56e57ca89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a1413c9a75349699083ba0c6769d966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ee126b4a3343f1be8fa8e1904840eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}